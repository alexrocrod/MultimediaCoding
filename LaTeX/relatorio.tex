\documentclass[a4paper, 11pt]{article}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{indentfirst}
\setlength{\parindent}{20pt}
\usepackage{amssymb}
\usepackage{float}

\graphicspath{ {./images/} }
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\begin{document}	
	\title{Project \# 3. Multimedia Coding }
	\author{{\small Alexandre Rodrigues (2039952)}}
	\date{\today}
	\maketitle
	
	\section{Introduction}
		This reports is dedicated to explain the usage of the LBG-split algorithm and how to implement it.
		The Linde-Buzo-Gray algorithm is a lossy coding technique, it uses vector quantization, meaning that a block of input samples (size L) is processed together.
		$\ldots$
	
	\section{Technical Approach}
		Each vector of size L samples can be defined as
		\begin{equation}
			x = [x_1, x_2, \ldots, x_L], x \in R^L
		\end{equation}
		where $x_i, i=1,2,\ldots,L$ are input samples. 
		Using $y_i $ as a codevector, 
		\begin{equation}
			B =  \{y_1, y_2, \ldots, y_L\}
		\end{equation} 
		is the set of reconstruction levels, i.e. the codebook, of size K.
		The decision cells will be
		\begin{equation}
			I_i \in R^L, i = 1, 2, \ldots, K, such that I_i intersect I_j = 0 \aleph i \neq j and union k i=1 I_i = R^L
		\end{equation} 
		
	
	\section{Results}
		There were several tests made to fully understand the performance of this method.
		
		There 3 different trainng sets used:
			1- All audio files from MC dataset
			2- Only the Say Nada song
			3- All Audio Files and All Popular Music
			
		\begin{table}[H]
			\centering
			\begin{tabular}{c|c|c}
				\textbf{Training set} & \textbf{N/K for L = 2} 	& \textbf{N/K for L = 4} \\ \hline
				All Audio 			& $ 1250 $ 					& $ 625 $	 			 \\ \hline
				Say Nada			& $ 1000 $ 				  	& $ 500 $ \\ \hline	
				All Music and Audio & $ 4844 $					& $	2422 $ \\
			\end{tabular}
			\caption{Training sets and their stats}
			\label{table:TrainSets}
		\end{table}
		
		\subsection{Training Performance}
		
			The tests were made using $ \epsilon = 0.01 $.
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Training set} & \textbf{2,2} 			& \textbf{2,4}			&  \textbf{4,1}				& \textbf{4,2} \\ \hline
					All Audio 			& $ 1.77 \times 10^{5} $ 	& $ 2.12 \times 10^{5} $& $4.75 \times 10^{5} $ 	& $ 2.85 \times 10^{5} $ \\ \hline
					Music: Say Nada		& $ 4.36 \times 10^{6} $ 	& $ 3.70 \times 10^{6} $& $ 1.63 \times 10^{7} $  	& $ 2.37 \times 10^{7} $ \\ \hline	
					All Music 			& $ ?? $					& $	?? $				& $ ?? $					& $	?? $ \\ \hline
					All Music and Audio & $ ?? $					& $	?? $				& $ ?? $					& $	?? $ \\
				\end{tabular}
				\caption{Distortion for each training set and each values of L and R}
				\label{table:TrainDist}
			\end{table}
			
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Training set}	 	& \textbf{2,2} & \textbf{2,4}	& \textbf{4,1} & \textbf{4,2}\\ \hline
					All Audio 					& $ 0.45 s $ 	& $ 101.49 s $	& $ 0.32 s $ 	& $ 84.79 5 s $	\\ \hline
					Music: Say Nada				& $ 0.73 s $	& $ 115.93 s $	& $ 0.39 s $ 	& $ 95.22 s $	\\ \hline	
					All Music					& $ ?? $		& $	?? $		& $ ?? $		& $	?? $		\\ \hline
					All Music and All Audio 	& $ ?? $		& $	?? $		& $ ?? $		& $	?? $		\\
				\end{tabular}
				\caption{Time for each training set and each values of L and R}
				\label{table:TrainTime}
			\end{table}
		
			We can see that (4,1) is clearly the fastest training.
			The training time is mostly dependent on the value K.
			The distortion is noticeably larger for L = 4.
			
		\subsection{Encoding Performance}
			In summary I got the following results:
			
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Encoded}	& \textbf{2,2} 	& \textbf{2,4}	& \textbf{4,1} & \textbf{4,2}\\ \hline
					70mono					& $ 1.20 s $ 	& $ 16.79 s $	& $ 0.60 s $ 	& $ 8.70 5 s $	\\ \hline
					Average Music			& $ 13.40 s $	& $ 193.61 s $	& $ 6.66 s $ 	& $ 104.92 s $	\\ \hline	
					Worst Case				& $ 18.40 s $	& $	232.67 s $	& $ 8.22 s $	& $	118.26 s $	\\ \hline
					Best Case				& $ 10.89 s $	& $	149.41 s $	& $ 5.52 s $	& $	91.04 s $	\\
				\end{tabular}
				\caption{Time for each training set and each values of L and R}
				\label{table:EncodeTime}
			\end{table}
			There is no noticeable difference when using All Audio or Say Nada as the training set.
			
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Encoded} & \textbf{2,2} 			& \textbf{2,4}			&  \textbf{4,1}				& \textbf{4,2} \\ \hline
					70mono			& $ 5 \times 10^{5} $ 		& $ 2 \times 10^{4} $	& $4.75 \times 10^{5} $ 	& $ 8 \times 10^{4} $ \\ \hline
					Average Music	& $ 3.72 \times 10^{6} $ 	& $ 5.77 \times 10^{5} $& $ 4.66 \times 10^{6} $  	& $ 8.45 \times 10^{5} $ \\ \hline	
					Worst Case 		& $ 1.15 \times 10^{7} $	& $	8.20 \times 10^{5} $& $ 1.36 \times 10^{7} $	& $ 2.74 \times 10^6 $ \\ \hline
					Best Case 		& $ 6.2 \times 10^{5} $		& $	3.10 \times 10^{4} $& $ 9.42 \times 10^{5} $ 	& $	1.07 \times 10^5 $ \\
				\end{tabular}
				\caption{Distortion for each training set and each values of L and R}
				\label{table:EncodeDist}
			\end{table}
		
			
			Encoding music files using All Audio as training se gives us in average 4 times the distortion when using Say Nada as the training set.
			For (2,4) we get actually 11 times the distortion.
			
		
		\subsection{Training set including the encoding object...}
			
	
	\section{Conclusions}
	
		1 - Using music as a training set is clearly superior when we will use the codebook to encode music files.
			Vice-versa is also valid, although less significantly. Encoding 70mono using All audio as training set produces in average half the distortion.
		
		2 - 
		
		3 - 
		
		4 - 
		
		
		
		
		
		
\end{document}



