\documentclass[a4paper, 11pt]{article}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{indentfirst}
\setlength{\parindent}{20pt}
\usepackage{amssymb}
\usepackage{float}

\graphicspath{ {./images/} }
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\begin{document}	
	\title{Project \# 3. Multimedia Coding }
	\author{{\small Alexandre Rodrigues (2039952)}}
	\date{\today}
	\maketitle
	
	\section{Introduction}
		This reports is dedicated to explain the usage of the LBG-split algorithm and how to implement it.
		The Linde-Buzo-Gray algorithm is a lossy coding technique, it uses vector quantization, meaning that a block of input samples (size L) is processed together.
		$\ldots$
	
	\section{Technical Approach}
		Each vector of size L samples can be defined as
		\begin{equation}
			x = [x_1, x_2, \ldots, x_L], x \in R^L
		\end{equation}
		where $x_i, i=1,2,\ldots,L$ are input samples. 
		Using $y_i $ as a codevector, 
		\begin{equation}
			B =  \{y_1, y_2, \ldots, y_L\}
		\end{equation} 
		is the set of reconstruction levels, i.e. the codebook, of size K.
		The decision cells will be
		\begin{equation}
			I_i \in R^L, i = 1, 2, \ldots, K, such that I_i intersect I_j = 0 \aleph i \neq j and union k i=1 I_i = R^L
		\end{equation} 
	
		\subsection{LBG}
			Since we do not know the probablity distributuion function of the input data $ f_x(x) $ we can use a training set
			\begin{equation}
				T = {x_1,\ldots,x_N}
			\end{equation},
			where $N$ should be considerably larger than the codebook size $ K $,$  N \le 500K $ for this work.
			
			The algorithm can then be defined as:
			\begin{enumerate}
				\item initial codebook
				\item optimal partition
				\item new codebook
				\item distortion
				\item terminate?
			\end{enumerate}
		
		\subsection{Split approach}
				This approach is based on starting a codebook as 
				\begin{equation}
					\{(1-\epsilon)y_{avg}, (1+\epsilon)y_{avg}\},
				\end{equation}
				where $ y_{avg} $ is the average of the vectors in the training set.
				The LBG algorithm is then applied to this codebook.
				The returning optimized codebook is split in the same way, i.e.
				\begin{equation}
					\{(1-\epsilon)y_i, (1+\epsilon)y_i, \ldots, (1-\epsilon)y_N, (1+\epsilon)y_N \}, 
				\end{equation}
				This implies that the codebook size will double in each iteration until we get the desired size K, $ N=2,4,8, \ldots,K $.			
		
	
	\section{Results}
		There were several tests made to fully understand the performance of this method.
		
		\subsection{Important Parameters}
			As required there are 4 scenrarios
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c}
					\textbf{L} 		& \textbf{R} 	& \textbf{K} 	\\ \hline
					$ 2 $			& $ 2 $ 		& $ 16 $	  	\\ \hline
					$ 2 $			& $ 4 $ 	  	& $ 256 $ 		\\ \hline	
					$ 4 $			& $ 1 $ 		& $ 16 $ 		\\ \hline	
					$ 4 $			& $ 2 $			& $	256 $ 		\\
				\end{tabular}
				\caption{Scenarios}
				\label{table:Scenarios}
			\end{table}
		where K is the codebook size
		\begin{equation}
			K = 2^{RL}.
		\end{equation} 
		
		
		\subsection{Training Sets}
			There 3 different trainng sets used:
			\begin{itemize}
				\item All audio files from MC dataset
				\item Only the Say Nada song
				\item All Audio Files and All Popular Music
			\end{itemize}
				
			Each training set was limited in size. 
			For each file I extracted the middle part to result in the following relative size (N/K).
				
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c}
					\textbf{Training set} & \textbf{N/K for L = 2} 	& \textbf{N/K for L = 4} \\ \hline
					All Audio 			& $ 1250 $ 					& $ 625 $	 			 \\ \hline
					Say Nada			& $ 1000 $ 				  	& $ 500 $ \\ \hline	
					All Music and Audio & $ 4844 $					& $	2422 $ \\
				\end{tabular}
				\caption{Relative Sizes of each Training Set}
				\label{table:TrainSets}
			\end{table}
			
			These sizes allowed fast enough codebook computation.
			Having training sets with and without including the encoding objects will benefit our comparison and possible conclusions.
				
		\subsection{Training Performance}
		
			The tests were made using $ \epsilon = 0.01 $.
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Training set} & \textbf{2,2} 			& \textbf{2,4}			&  \textbf{4,1}				& \textbf{4,2} \\ \hline
					All Audio 			& $ 1.51 \times 10^{5} $ 	& $ 3.53 \times 10^{5} $& $4.48 \times 10^{5} $ 	& $ 3.15 \times 10^{6} $ \\ \hline
					Music: Say Nada		& $ 4.36 \times 10^{6} $ 	& $ ?? \times 10^{6} $	& $ 3.31 \times 10^{7} $  	& $ ??\times 10^{7} $ \\ \hline
					All Music and Audio & $ 2.47 \times 10^{6} $	& $	?? \times 10^{6}$	& $ 9.70 \times 10^{6} $	& $	?? \times 10^{7} $ \\
				\end{tabular}
				\caption{Distortion for each training set and each values of L and R}
				\label{table:TrainDist}
			\end{table}
			
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Training set}	 	& \textbf{2,2} & \textbf{2,4}	& \textbf{4,1} & \textbf{4,2}\\ \hline
					All Audio 					& $ 1.29 s $ 	& $ 179.09 s $	& $ 0.45 s $ 	& $ 86.02 5 s $	\\ \hline
					Music: Say Nada				& $ 0.75 s $	& $  s $	& $ 0.47 s $ 	& $  s $	\\ \hline
					All Music and All Audio 	& $ 2.57 s $	& $	?? $		& $ 1.21 s $	& $	?? $		\\
				\end{tabular}
				\caption{Time for each training set and each values of L and R}
				\label{table:TrainTime}
			\end{table}
		
			We can see that (4,1) is clearly the fastest training.
			The training time is mostly dependent on the value K.
			The distortion is noticeably larger for L = 4.
			
		\subsection{Encoding Performance}
			In summary I got the following results:
			
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Encoded}	& \textbf{2,2} 	& \textbf{2,4}	& \textbf{4,1} & \textbf{4,2}\\ \hline
					70mono					& $ 1.20 s $ 	& $ 16.79 s $	& $ 0.60 s $ 	& $ 8.70 5 s $	\\ \hline
					Average Music			& $ 13.40 s $	& $ 193.61 s $	& $ 6.66 s $ 	& $ 104.92 s $	\\ \hline	
					Worst Case				& $ 18.40 s $	& $	232.67 s $	& $ 8.22 s $	& $	118.26 s $	\\ \hline
					Best Case				& $ 10.89 s $	& $	149.41 s $	& $ 5.52 s $	& $	91.04 s $	\\
				\end{tabular}
				\caption{Time for each training set and each values of L and R}
				\label{table:EncodeTime}
			\end{table}
			There is no noticeable difference when using All Audio or Say Nada as the training set.
			
			\begin{table}[H]
				\centering
				\begin{tabular}{c|c|c|c|c}
					\textbf{Encoded} & \textbf{2,2} 			& \textbf{2,4}			&  \textbf{4,1}				& \textbf{4,2} \\ \hline
					70mono			& $ 5 \times 10^{5} $ 		& $ 2 \times 10^{4} $	& $4.75 \times 10^{5} $ 	& $ 8 \times 10^{4} $ \\ \hline
					Average Music	& $ 3.72 \times 10^{6} $ 	& $ 5.77 \times 10^{5} $& $ 4.66 \times 10^{6} $  	& $ 8.45 \times 10^{5} $ \\ \hline	
					Worst Case 		& $ 1.15 \times 10^{7} $	& $	8.20 \times 10^{5} $& $ 1.36 \times 10^{7} $	& $ 2.74 \times 10^6 $ \\ \hline
					Best Case 		& $ 6.2 \times 10^{5} $		& $	3.10 \times 10^{4} $& $ 9.42 \times 10^{5} $ 	& $	1.07 \times 10^5 $ \\
				\end{tabular}
				\caption{Distortion for each training set and each values of L and R}
				\label{table:EncodeDist}
			\end{table}
		
			
			Encoding music files using All Audio as training se gives us in average 4 times the distortion when using Say Nada as the training set.
			For (2,4) we get actually 11 times the distortion.
			
		
		\subsection{Training set including the encoding object...}
			
	
	\section{Conclusions}
	
		1 - Using music as a training set is clearly superior when we will use the codebook to encode music files.
			Vice-versa is also valid, although less significantly. Encoding 70mono using All audio as training set produces in average half the distortion.
		
		2 - 
		
		3 - 
		
		4 - 
		
		
		
		
		
		
\end{document}



